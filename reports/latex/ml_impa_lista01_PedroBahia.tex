\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[brazilian]{babel}
\usepackage{url, hyperref}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}

\title{Lista 01 de Machine Learning}
\author{Pedro Barbosa Bahia}
\date{IMPA, Verão 2026}

\begin{document}

% Configure Python syntax highlighting
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    tabsize=4
}

\maketitle

\tableofcontents
\pagebreak

\section{Exercício 1a}

\textbf{Falso.} A proximidade entre $\varepsilon_{\text{treino}}$ e $\varepsilon_{\text{teste}}$ pode dar informações sobre o ajuste do modelo aos dados de treino.

Caso $\varepsilon_{\text{treino}}$ seja próximo ao $\varepsilon_{\text{teste}}$, o modelo pode estar \textit{subajustado}, de modo que aumentar a complexidade poderia melhorar sua performance ainda mais.

De maneira análoga, caso o $\varepsilon_{\text{treino}}$ seja menor que o $\varepsilon_{\text{teste}}$, o modelo estará \textit{sobreajustado}, com redução de complexidade podendo resultar em melhoras.

\newpage

\section{Exercício 1b}

\textbf{Verdadeiro.} A distribuição $t$ surge do fato de $\mathcal{N}(0,1)$ dividido por $\sqrt{\frac{K}{N}}$ ter distribuição $t$ com $N$ graus de liberdade.

No nosso caso, $\mathcal{N}(0,1)$ é a distribuição de $\frac{\hat{\beta_1} - \beta_1}{\text{Var}(\hat{\beta_1})}$. Isso é normal, pois $\hat{\beta_1}$ é normal.

Isso, por sua vez, vem do fato de $\hat{\beta}$ ser resultante de uma combinação linear de gaussianas, no caso, $\varepsilon \sim \mathcal{N}(0, \sigma^2)$.

\newpage

\section{Exercício 1c}
\textbf{Falso.}
Considerando a classe $k = 0$ como as transações fraudulentas, o objetivo do modelo pode ser interpretado como:

\[
    \sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]} = 0
\]

Não há restrições entretanto em relação às transações legítimas, ou seja, para:
\[
    \sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}
\]

Dado um modelo de acurácia $(1- \varepsilon)$, têm-se que
\[ 1- \frac{1}{n} \left[\sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]} + \sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}\right] = 1 - \varepsilon \]

Dado uma acurácia \[1 - epsilon\], há infinitos valores de $\sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]}$ e $\sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}$ que resolvem essa equação e, portanto, o valor da acurácia não é informativo para o erro individual das classes.
Assim, apenas com a acurácias dos Modelos 1 e 2 não é possível determinar qual modelo tem menor erro em transações fraudulentas.


\newpage

\section{Exercício 1d}
\textbf{Falso}


\[ L_{ridge}(\beta) = (Y - Yhat)^T(Y - \hat{Y}) + \lambda \beta^T \beta \]
\[ L_{ridge}(\beta)  = (Y - X\beta)^T(Y - X\beta) + \lambda \beta^T \beta \]

\[L_{linear}(\beta) = (Y - Yhat)^T(Y - \hat{Y})\]

Caso $\lambda = 0$, temos que $L_{ridge}(\beta) = L_{linear}(\beta)$.
Logo, a performance de ambos os modelos será a mesma. Então para o case de $\lambda = 0$, a afirmação é falsa.

\newpage


\section{Exercício 2}

\subsection{Parte d}

\subsubsection{i) Geração dos Dados Heteroscedásticos}

Primeiramente, geramos os dados com heterocedasticidade usando a matriz de covariância $\Sigma$ diagonal:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import matplotlib.pyplot as plt

n = 50
Sigma = np.diag([10 ** ((i - 20) / 5) for i in range(1, n + 1)])
np.random.seed(0)
X = np.array([np.ones(n), np.random.normal(0, 1, n)]).T
beta = np.array([1, 0.25])
epsilon = np.random.multivariate_normal(np.zeros(n), Sigma)
y = X @ beta + epsilon
\end{lstlisting}

A Figura \ref{fig:heteroscedastic_data} mostra os dados gerados com heterocedasticidade:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/heteroscedastic_data_scatter.png}
    \caption{Dados heteroscedásticos gerados}
    \label{fig:heteroscedastic_data}
\end{figure}

A Figura \ref{fig:sigma_variances} mostra os elementos diagonais da matriz $\Sigma$, evidenciando a heterocedasticidade:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/sigma_diagonal_variances.png}
    \caption{Elementos diagonais da matriz $\Sigma$}
    \label{fig:sigma_variances}
\end{figure}

As Figuras \ref{fig:errors_hist} e \ref{fig:errors_values} mostram a distribuição e valores dos erros:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/errors_histogram.png}
    \caption{Histograma dos erros}
    \label{fig:errors_hist}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/error_values_by_index.png}
    \caption{Valores dos erros por índice}
    \label{fig:errors_values}
\end{figure}

\subsubsection{ii) Estimadores de Mínimos Quadrados}

Implementamos tanto o estimador de mínimos quadrados ordinários quanto o estimador generalizado que considera a matriz de covariância $\Sigma$:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def beta_ordinary(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    """
    Compute the ordinary least squares estimator.
    """
    beta = np.linalg.inv(X.T @ X) @ X.T @ Y
    return beta

def beta_sigma(X: np.ndarray, Y: np.ndarray, Sigma: np.ndarray) -> np.ndarray:
    """
    Compute the generalized least squares estimator considering 
    the covariance matrix Sigma.
    """
    Sigma_1 = np.linalg.inv(Sigma)
    beta = np.linalg.inv(X.T @ Sigma_1 @ X) @ X.T @ Sigma_1 @ Y
    return beta

beta_hat_ordinary = beta_ordinary(X, y)
beta_hat_sigma = beta_sigma(X, y, Sigma)

print("True Beta:", beta)
print("Ordinary Beta:", beta_hat_ordinary)
print("Beta Sigma:", beta_hat_sigma)
\end{lstlisting}

Os resultados mostram que o estimador generalizado (que considera $\Sigma$) tem menor erro em relação aos parâmetros verdadeiros.

\subsubsection{iii) Cálculo de p-valores para Mínimos Quadrados Ordinários}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def p_value_ordinary_least_square(X: np.ndarray, Y: np.ndarray, 
                                 beta_ordinary_hat: np.ndarray, j: int) -> float:
    """
    Compute the p-value for the j-th coefficient of the ordinary 
    least squares estimator.
    """
    Y_hat = X @ beta_ordinary_hat
    n, p = X.shape
    dof = n - p
    errors = Y - Y_hat
    beta_j = beta_ordinary_hat[j]
    
    # Z statistic
    x_j_var = (np.linalg.inv(X.T @ X))[j, j]
    Z = beta_j / np.sqrt(x_j_var)
    
    # Estimate of sigma^2
    sigma2_hat = (1 / dof) * (errors.T @ errors)
    
    # t statistic and p-value
    t_statistics = Z / np.sqrt(sigma2_hat)
    t_statistics = np.abs(t_statistics)
    p_value = 2 * (1 - scipy.stats.t.cdf(t_statistics, dof))
    
    return p_value
\end{lstlisting}

\subsubsection{iv) Estatística Z para o Estimador Generalizado}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def calculate_Z_sigma(X: np.ndarray, Sigma: np.ndarray, 
                     Beta_sigma: np.ndarray, j: int) -> float:
    """
    Compute the Z statistic for the j-th coefficient of the 
    generalized least squares estimator.
    """
    Sigma_inv = np.linalg.inv(Sigma)
    den = np.linalg.inv(X.T @ Sigma_inv @ X)
    den = den[j, j]
    Z = Beta_sigma[j] / (np.sqrt(den))
    return Z
\end{lstlisting}

\subsubsection{v) P-valores para o Estimador Generalizado}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def p_value_generalized_least_square(X: np.ndarray, Y: np.ndarray,
                                    Sigma: np.ndarray, 
                                    beta_ordinary_hat: np.ndarray, j: int) -> float:
    """
    Compute the p-value for the j-th coefficient of the 
    generalized least squares estimator.
    """
    Y_hat = X @ beta_ordinary_hat
    n, p = X.shape
    dof = n - p
    errors = Y - Y_hat
    
    # Z statistic
    Z_sigma = calculate_Z_sigma(X, Sigma, beta_hat_sigma, j)
    
    # Estimate of sigma^2
    inverse_Sigma = np.linalg.inv(Sigma)
    sigma2_hat = (1 / dof) * (errors.T @ inverse_Sigma @ errors)
    
    # t statistic and p-value
    t_statistics = Z_sigma / np.sqrt(sigma2_hat)
    t_statistics = np.abs(t_statistics)
    p_value = 2 * (1 - scipy.stats.t.cdf(t_statistics, dof))
    
    return p_value
\end{lstlisting}

Esta implementação permite comparar os dois estimadores e calcular a significância estatística dos coeficientes em ambos os casos.

\subsubsection{vi) Resultados Numéricos}

Executando o código implementado, obtemos os seguintes resultados:

\textbf{Comparação dos Estimadores:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [1.0, 0.25]$
    \item Estimador Ordinário: $\hat{\beta}_{OLS} = [-34.463, 6.948]$
    \item Estimador Generalizado: $\hat{\beta}_{\Sigma} = [1.019, 0.244]$
\end{itemize}

\textbf{Erro Quadrático dos Estimadores:}
\begin{itemize}
    \item $\|\beta - \hat{\beta}_{OLS}\|_2^2 = 1302.51$
    \item $\|\beta - \hat{\beta}_{\Sigma}\|_2^2 = 0.000387$
\end{itemize}

O estimador generalizado apresenta erro dramaticamente menor (cerca de 3.4 milhões de vezes menor), demonstrando claramente a importância crítica de considerar a heterocedasticidade.

\textbf{Testes de Hipótese - Mínimos Quadrados Ordinários:}
\begin{itemize}
    \item p-valor para $\beta_0$: 0.1544
    \item p-valor para $\beta_1$: 0.7422
\end{itemize}

Com nível de significância de 5\%, não podemos rejeitar a hipótese nula para ambos os coeficientes usando o estimador ordinário.

\textbf{Testes de Hipótese - Estimador Generalizado:}
\begin{itemize}
    \item Estatística Z para $\beta_0$: 73.67
    \item p-valor para $\beta_0$: $< 10^{-15}$ (praticamente zero)
    \item p-valor para $\beta_1$: $< 10^{-15}$ (praticamente zero)
\end{itemize}

Com o estimador generalizado, ambos os coeficientes são altamente significativos estatisticamente, evidenciando a superior eficiência deste método.

\textbf{Conclusões:}
\begin{enumerate}
    \item O estimador de mínimos quadrados ordinários (OLS) falha completamente na presença de heterocedasticidade severa, fornecendo estimativas muito distantes dos valores verdadeiros ($\hat{\beta}_0 = -34.46$ vs $\beta_0 = 1.0$).

    \item O estimador generalizado (GLS) é extremamente superior, com erro cerca de 3.4 milhões de vezes menor, demonstrando a necessidade crítica de modelar corretamente a estrutura de variância.

    \item Os testes de significância baseados no OLS são não-confiáveis, falhando em detectar coeficientes que são claramente diferentes de zero.

    \item O estimador GLS fornece testes estatísticos apropriados, detectando corretamente a significância dos parâmetros.

    \item Este exemplo ilustra dramaticamente por que a correção para heterocedasticidade não é apenas uma melhoria técnica, mas uma necessidade fundamental para análise estatística válida.
\end{enumerate}

\newpage


\section{Exercício 3}

\subsection{Parte f}

\subsubsection{i) Implementação dos Estimadores}

Neste exercício, comparamos estimadores baseados em diferentes suposições sobre a distribuição dos erros. Implementamos estimadores que minimizam diferentes funções de perda:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import scipy
import matplotlib.pyplot as plt

def beta_ordinary(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    """
    Compute the ordinary least squares estimator.
    """
    beta = np.linalg.inv(X.T @ X) @ X.T @ Y
    return beta

def calculate_beta_hat(X: np.ndarray, Y: np.ndarray, error_distribution: str) -> np.ndarray:
    """
    Compute the estimator beta_hat based on the specified error distribution.
    """
    np.random.seed(0)
    p = X.shape[1]
    
    if error_distribution == "gaussian":
        def loss_function(beta):
            return np.sum((Y - X @ beta) ** 2)
    elif error_distribution == "laplacian":
        def loss_function(beta):
            return np.sum(np.abs(Y - X @ beta))
    else:
        raise ValueError("Unsupported error distribution")
    
    beta_0 = np.random.uniform(size=p)
    beta_hat = scipy.optimize.minimize(loss_function, beta_0)
    return beta_hat["x"]
\end{lstlisting}

Geramos dados sintéticos para testar os estimadores:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
np.random.seed(1)
beta = np.array([-1.5, 2.0])
input_range = np.linspace(-1, 1, 100)
X = np.vstack([np.ones(100), input_range]).T
y = X @ beta + np.random.normal(0, 0.3, 100)
\end{lstlisting}

A Figura \ref{fig:data_scatter_q3} mostra os dados gerados:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/data_scatter.png}
    \caption{Dados sintéticos para comparação dos estimadores}
    \label{fig:data_scatter_q3}
\end{figure}

As Figuras \ref{fig:errors_hist_q3} e \ref{fig:errors_values_q3} mostram a análise dos erros:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/errors_histogram.png}
    \caption{Histograma dos erros}
    \label{fig:errors_hist_q3}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/error_values_by_index.png}
    \caption{Valores dos erros por índice}
    \label{fig:errors_values_q3}
\end{figure}

\subsubsection{ii) Comparação dos Estimadores}

Calculamos os estimadores para ambas as distribuições de erro:

\textbf{Resultados sem Outliers:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [-1.5, 2.0]$
    \item Estimador Gaussiano (minimize): $\hat{\beta}_{Gauss} = [-1.482, 2.050]$
    \item Estimador Gaussiano (forma fechada): $\hat{\beta}_{OLS} = [-1.482, 2.050]$
    \item Estimador Laplaciano: $\hat{\beta}_{Lap} = [-1.498, 2.082]$
\end{itemize}

\textbf{Análise de Erro (Norma L2):}
\begin{itemize}
    \item Erro do Estimador Gaussiano: $0.053$
    \item Erro do Estimador Laplaciano: $0.082$
\end{itemize}

Sem outliers, o estimador gaussiano (mínimos quadrados) tem melhor performance, como esperado quando os erros seguem distribuição normal.

A Figura \ref{fig:regression_fits} compara visualmente os ajustes:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/regression_fits_comparison.png}
    \caption{Comparação dos ajustes de regressão sem outliers}
    \label{fig:regression_fits}
\end{figure}

\subsubsection{iii) Robustez a Outliers}

Para testar a robustez, adicionamos um outlier extremo ao dataset:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
# Regenerate data and add outlier
y[80] = 10  # Extreme outlier
\end{lstlisting}

\textbf{Resultados com Outlier:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [-1.5, 2.0]$
    \item Estimador Gaussiano com outlier: $\hat{\beta}_{Gauss} = [-1.378, 2.237]$
    \item Estimador Laplaciano com outlier: $\hat{\beta}_{Lap} = [-1.498, 2.083]$
\end{itemize}

\textbf{Análise de Erro com Outlier (Norma L2):}
\begin{itemize}
    \item Erro do Estimador Gaussiano: $0.266$ (aumento de 5x)
    \item Erro do Estimador Laplaciano: $0.083$ (praticamente inalterado)
\end{itemize}

A Figura \ref{fig:regression_fits_outlier} mostra o impacto do outlier:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/regression_fits_with_outlier.png}
    \caption{Comparação dos ajustes de regressão com outlier}
    \label{fig:regression_fits_outlier}
\end{figure}

\textbf{Conclusões:}
\begin{enumerate}
    \item \textbf{Eficiência}: Quando os erros são gaussianos e não há outliers, o estimador de mínimos quadrados (gaussiano) é mais eficiente.

    \item \textbf{Robustez}: O estimador laplaciano é significativamente mais robusto a outliers, mantendo sua performance praticamente inalterada mesmo com outliers extremos.

    \item \textbf{Trade-off}: Existe um trade-off entre eficiência (mínimos quadrados) e robustez (estimador laplaciano). A escolha depende das características esperadas dos dados.

    \item \textbf{Aplicação Prática}: Em situações onde outliers são esperados ou a distribuição dos erros tem caudas pesadas, o estimador laplaciano (regressão com norma L1) é preferível.

    \item \textbf{Impacto Dramático}: Um único outlier pode degradar significativamente a performance do estimador gaussiano (aumento de 5x no erro), enquanto o estimador laplaciano permanece praticamente inalterado.
\end{enumerate}

\newpage


\section{Exercício 4a}

\subsection{Implementação dos Algoritmos de Classificação}

Neste exercício, implementamos e comparamos cinco diferentes algoritmos de classificação usando o dataset de futebol:

\begin{itemize}
    \item \textbf{LDA} (Linear Discriminant Analysis)
    \item \textbf{QDA} (Quadratic Discriminant Analysis)
    \item \textbf{LR} (Logistic Regression)
    \item \textbf{NB} (Naive Bayes Gaussiano)
    \item \textbf{kNN} (k-Nearest Neighbors)
\end{itemize}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
from sklearn.linear_model import LogisticRegression as LR
from sklearn.naive_bayes import GaussianNB as NB
from sklearn.neighbors import KNeighborsClassifier as kNN
from sklearn import preprocessing

# Load and prepare data
df = pd.read_csv("../data/soccer.csv")
X = df.drop("target", axis=1)
y = df[["target"]]

# Split dataset
X_train, y_train = X.iloc[:2560], y.iloc[:2560]
X_test, y_test = X.iloc[2560:], y.iloc[2560:]

# Remove categorical variables and standardize
X_train = X_train.drop(["home_team", "away_team"], axis=1)
X_test = X_test.drop(["home_team", "away_team"], axis=1)
scaler = preprocessing.StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{lstlisting}

\textbf{Informações do Dataset:}
\begin{itemize}
    \item Amostras de treino: 2560
    \item Amostras de teste: 640
    \item Features após pré-processamento: 11 (removendo variáveis categóricas)
\end{itemize}

\newpage

\section{Exercício 4b}

\subsection{Treinamento e Avaliação dos Modelos}

Implementamos um loop para treinar todos os modelos e comparar suas performances:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
models_to_test = [LDA, QDA, LR, NB, kNN]
results_dict = {}

for model_type in models_to_test:
    model_name = model_type.__name__
    params = {}
    if model_type in [LDA, QDA]:
        params.update({"store_covariance": True})
    
    results_dict[model_name] = {}
    cls = model_type(**params)
    cls.fit(X_train, y_train.values.ravel())
    
    # Store predictions and model
    results_dict[model_name]["in_sample_predictions"] = cls.predict(X_train)
    results_dict[model_name]["test_predictions"] = cls.predict(X_test)
    results_dict[model_name]["model"] = cls
\end{lstlisting}

\subsection{Comparação de Performance dos Modelos}

\subsubsection{Análise Geral dos Algoritmos}

A Figura \ref{fig:models_comparison} compara os erros de treinamento e teste para todos os modelos:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/4/models_error_comparison.png}
    \caption{Comparação dos erros de treinamento vs teste para diferentes modelos}
    \label{fig:models_comparison}
\end{figure}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
for model_name in models_to_test:
    model_type_name = model_name.__name__
    in_sample_predictions = results_dict[model_type_name]["in_sample_predictions"]
    test_predictions = results_dict[model_type_name]["test_predictions"]
    
    train_error = np.mean(in_sample_predictions != y_train.values.ravel())
    test_error = np.mean(test_predictions != y_test.values.ravel())
    
    plt.scatter(train_error, test_error, label=model_type_name)
\end{lstlisting}

\subsubsection{Análise Específica do k-NN}

A Figura \ref{fig:knn_comparison} mostra como a performance do k-NN varia com diferentes valores de k:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/4/knn_error_comparison.png}
    \caption{Erros do k-NN para diferentes números de vizinhos (K=1 a K=10)}
    \label{fig:knn_comparison}
\end{figure}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
for k in range(1, 11):
    model = kNN(n_neighbors=k)
    model.fit(X_train, y_train.values.ravel())
    
    in_sample_predictions_k = model.predict(X_train)
    test_predictions_k = model.predict(X_test)
    
    train_error = np.mean(in_sample_predictions_k != y_train.values.ravel())
    test_error = np.mean(test_predictions_k != y_test.values.ravel())
    
    plt.scatter(train_error, test_error, label=f"K = {k}", marker=f"${k}$")
\end{lstlisting}

\subsection{Análise dos Coeficientes dos Modelos}

\textbf{Linear Discriminant Analysis (LDA):}
\begin{itemize}
    \item Coeficientes: $[0.81, -0.26, -0.026, 0.017, 0.23, 0.069, 0.37, -0.050, -0.083, 0.043, 0.047]$
    \item Intercepto: $0.109$
    \item Utiliza covariância comum entre as classes
\end{itemize}

\textbf{Logistic Regression:}
\begin{itemize}
    \item Coeficientes: $[0.87, -0.29, -0.020, 0.056, 0.26, 0.078, 0.40, -0.053, -0.078, 0.063, 0.047]$
    \item Intercepto: $0.128$
    \item Coeficientes similares ao LDA, indicando estrutura linear nos dados
\end{itemize}

\textbf{Gaussian Naive Bayes:}
\begin{itemize}
    \item Assume independência condicional entre features
    \item Médias das classes: $[-0.49, 0.28, 0.27, -0.30, -0.31, 0.23, -0.32, -0.18, 0.25, 0.17, 0.031]$ (Classe 0)
    \item Variâncias por feature calculadas separadamente para cada classe
\end{itemize}

\subsection{Conclusões}

\begin{enumerate}
    \item \textbf{Performance Geral}: Todos os modelos apresentaram performance similar, sugerindo que o problema tem estrutura linear bem definida.

    \item \textbf{Overfitting}: O k-NN com K=1 mostra clear overfitting (erro de treino muito baixo, erro de teste alto), enquanto valores maiores de K generalizam melhor.

    \item \textbf{Estabilidade}: LDA, QDA e Logistic Regression apresentaram performance consistente entre treino e teste.

    \item \textbf{Complexidade}: QDA, ao modelar covariâncias separadas por classe, não mostrou melhoria significativa sobre LDA, indicando que a suposição de covariância comum é adequada.

    \item \textbf{Naive Bayes}: Manteve performance competitiva mesmo com a forte suposição de independência, sugerindo que as correlações entre features não são críticas para este problema.

    \item \textbf{k-NN}: A performance otimizada ocorre com K entre 3-7, balanceando bias e variância.
\end{enumerate}

\newpage


\end{document}