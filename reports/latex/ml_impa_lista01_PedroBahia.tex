\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[brazilian]{babel}
\usepackage{url, hyperref}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{float}

\title{Lista 01 de Machine Learning}
\author{Pedro Barbosa Bahia}
\date{IMPA, Verão 2026}

\begin{document}

% Configure Python syntax highlighting
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green},
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    tabsize=4
}

\maketitle

\tableofcontents
\pagebreak

\section{Exercício 1a}

\textbf{Falso.} A proximidade entre $\varepsilon_{\text{treino}}$ e $\varepsilon_{\text{teste}}$ pode dar informações sobre o ajuste do modelo aos dados de treino.

Caso $\varepsilon_{\text{treino}}$ seja próximo ao $\varepsilon_{\text{teste}}$, o modelo pode estar \textit{subajustado}, de modo que aumentar a complexidade poderia melhorar sua performance ainda mais.

De maneira análoga, caso o $\varepsilon_{\text{treino}}$ seja menor que o $\varepsilon_{\text{teste}}$, o modelo estará \textit{sobreajustado}, com redução de complexidade podendo resultar em melhoras.

\newpage

\section{Exercício 1b}

\textbf{Verdadeiro.} A distribuição $t$ surge do fato de $\mathcal{N}(0,1)$ dividido por $\sqrt{\frac{K}{N}}$ ter distribuição $t$ com $N$ graus de liberdade.

No nosso caso, $\mathcal{N}(0,1)$ é a distribuição de $\frac{\hat{\beta_1} - \beta_1}{\text{Var}(\hat{\beta_1})}$. Isso é normal, pois $\hat{\beta_1}$ é normal.

Isso, por sua vez, vem do fato de $\hat{\beta}$ ser resultante de uma combinação linear de gaussianas, no caso, $\varepsilon \sim \mathcal{N}(0, \sigma^2)$.

\newpage

\section{Exercício 1c}
\textbf{Falso.}
Considerando a classe $k = 0$ como as transações fraudulentas, o objetivo do modelo pode ser interpretado como:

\[
    \sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]} = 0
\]

Não há restrições entretanto em relação às transações legítimas, ou seja, para:
\[
    \sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}
\]

Dado um modelo de acurácia $(1- \varepsilon)$, têm-se que
\[ 1- \frac{1}{n} \left[\sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]} + \sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}\right] = 1 - \varepsilon \]

Dado uma acurácia \[1 - epsilon\], há infinitos valores de $\sum_{y_i \in k=0} \mathbf{1}_{[y_i \neq \hat{y_i}]}$ e $\sum_{y_i \in k=1} \mathbf{1}_{[y_i \neq \hat{y_i}]}$ que resolvem essa equação e, portanto, o valor da acurácia não é informativo para o erro individual das classes.
Assim, apenas com a acurácias dos Modelos 1 e 2 não é possível determinar qual modelo tem menor erro em transações fraudulentas.


\newpage

\section{Exercício 1d}
\textbf{Falso}


\[ L_{ridge}(\beta) = (Y - Yhat)^T(Y - \hat{Y}) + \lambda \beta^T \beta \]
\[ L_{ridge}(\beta)  = (Y - X\beta)^T(Y - X\beta) + \lambda \beta^T \beta \]

\[L_{linear}(\beta) = (Y - Yhat)^T(Y - \hat{Y})\]

Caso $\lambda = 0$, temos que $L_{ridge}(\beta) = L_{linear}(\beta)$.
Logo, a performance de ambos os modelos será a mesma. Então para o case de $\lambda = 0$, a afirmação é falsa.

\newpage


\section{Exercício 2d}

\subsection{i) Gera\c{c}\~ao dos Dados Heteroscedásticos}

Primeiramente, geramos os dados com heterocedasticidade usando a matriz de covariância $\Sigma$ diagonal:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import matplotlib.pyplot as plt

n = 50
Sigma = np.diag([10 ** ((i - 20) / 5) for i in range(1, n + 1)])
np.random.seed(0)
X = np.array([np.ones(n), np.random.normal(0, 1, n)]).T
beta = np.array([1, 0.25])
epsilon = np.random.multivariate_normal(np.zeros(n), Sigma)
y = X @ beta + epsilon
\end{lstlisting}

A Figura \ref{fig:heteroscedastic_data} mostra os dados gerados com heterocedasticidade:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/heteroscedastic_data_scatter.png}
    \caption{Dados heteroscedásticos gerados}
    \label{fig:heteroscedastic_data}
\end{figure}

A Figura \ref{fig:sigma_variances} mostra os elementos diagonais da matriz $\Sigma$, evidenciando a heterocedasticidade:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/sigma_diagonal_variances.png}
    \caption{Elementos diagonais da matriz $\Sigma$}
    \label{fig:sigma_variances}
\end{figure}

As Figuras \ref{fig:errors_hist} e \ref{fig:errors_values} mostram a distribuição e valores dos erros:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/errors_histogram.png}
    \caption{Histograma dos erros}
    \label{fig:errors_hist}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/2/error_values_by_index.png}
    \caption{Valores dos erros por índice}
    \label{fig:errors_values}
\end{figure}

\subsection{ii) Estimadores de Mínimos Quadrados}

Implementamos tanto o estimador de mínimos quadrados ordinários quanto o estimador generalizado que considera a matriz de covariância $\Sigma$:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def beta_ordinary(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    """
    Compute the ordinary least squares estimator.
    """
    beta = np.linalg.inv(X.T @ X) @ X.T @ Y
    return beta

def beta_sigma(X: np.ndarray, Y: np.ndarray, Sigma: np.ndarray) -> np.ndarray:
    """
    Compute the generalized least squares estimator considering 
    the covariance matrix Sigma.
    """
    Sigma_1 = np.linalg.inv(Sigma)
    beta = np.linalg.inv(X.T @ Sigma_1 @ X) @ X.T @ Sigma_1 @ Y
    return beta

beta_hat_ordinary = beta_ordinary(X, y)
beta_hat_sigma = beta_sigma(X, y, Sigma)

print("True Beta:", beta)
print("Ordinary Beta:", beta_hat_ordinary)
print("Beta Sigma:", beta_hat_sigma)
\end{lstlisting}

Os resultados mostram que o estimador generalizado (que considera $\Sigma$) tem menor erro em relação aos parâmetros verdadeiros.

\subsection{iii) Cálculo de p-valores para Mínimos Quadrados Ordinários}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def p_value_ordinary_least_square(X: np.ndarray, Y: np.ndarray, 
                                 beta_ordinary_hat: np.ndarray, j: int) -> float:
    """
    Compute the p-value for the j-th coefficient of the ordinary 
    least squares estimator.
    """
    Y_hat = X @ beta_ordinary_hat
    n, p = X.shape
    dof = n - p
    errors = Y - Y_hat
    beta_j = beta_ordinary_hat[j]
    
    # Z statistic
    x_j_var = (np.linalg.inv(X.T @ X))[j, j]
    Z = beta_j / np.sqrt(x_j_var)
    
    # Estimate of sigma^2
    sigma2_hat = (1 / dof) * (errors.T @ errors)
    
    # t statistic and p-value
    t_statistics = Z / np.sqrt(sigma2_hat)
    t_statistics = np.abs(t_statistics)
    p_value = 2 * (1 - scipy.stats.t.cdf(t_statistics, dof))
    
    return p_value
\end{lstlisting}

\subsection{iv) Estatística Z para o Estimador Generalizado}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def calculate_Z_sigma(X: np.ndarray, Sigma: np.ndarray, 
                     Beta_sigma: np.ndarray, j: int) -> float:
    """
    Compute the Z statistic for the j-th coefficient of the 
    generalized least squares estimator.
    """
    Sigma_inv = np.linalg.inv(Sigma)
    den = np.linalg.inv(X.T @ Sigma_inv @ X)
    den = den[j, j]
    Z = Beta_sigma[j] / (np.sqrt(den))
    return Z
\end{lstlisting}

\subsection{v) P-valores para o Estimador Generalizado}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def p_value_generalized_least_square(X: np.ndarray, Y: np.ndarray,
                                    Sigma: np.ndarray, 
                                    beta_ordinary_hat: np.ndarray, j: int) -> float:
    """
    Compute the p-value for the j-th coefficient of the 
    generalized least squares estimator.
    """
    Y_hat = X @ beta_ordinary_hat
    n, p = X.shape
    dof = n - p
    errors = Y - Y_hat
    
    # Z statistic
    Z_sigma = calculate_Z_sigma(X, Sigma, beta_hat_sigma, j)
    
    # Estimate of sigma^2
    inverse_Sigma = np.linalg.inv(Sigma)
    sigma2_hat = (1 / dof) * (errors.T @ inverse_Sigma @ errors)
    
    # t statistic and p-value
    t_statistics = Z_sigma / np.sqrt(sigma2_hat)
    t_statistics = np.abs(t_statistics)
    p_value = 2 * (1 - scipy.stats.t.cdf(t_statistics, dof))
    
    return p_value
\end{lstlisting}

Esta implementação permite comparar os dois estimadores e calcular a significância estatística dos coeficientes em ambos os casos.

\subsection{vi) Resultados Numéricos}

Executando o código implementado, obtemos os seguintes resultados:

\textbf{Comparação dos Estimadores:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [1.0, 0.25]$
    \item Estimador Ordinário: $\hat{\beta}_{OLS} = [-34.463, 6.948]$
    \item Estimador Generalizado: $\hat{\beta}_{\Sigma} = [1.019, 0.244]$
\end{itemize}

\textbf{Erro Quadrático dos Estimadores:}
\begin{itemize}
    \item $\|\beta - \hat{\beta}_{OLS}\|_2^2 = 1302.51$
    \item $\|\beta - \hat{\beta}_{\Sigma}\|_2^2 = 0.000387$
\end{itemize}

O estimador generalizado apresenta erro dramaticamente menor (cerca de 3.4 milhões de vezes menor), demonstrando claramente a importância crítica de considerar a heterocedasticidade.

\textbf{Testes de Hipótese - Mínimos Quadrados Ordinários:}
\begin{itemize}
    \item p-valor para $\beta_0$: 0.1544
    \item p-valor para $\beta_1$: 0.7422
\end{itemize}

Com nível de significância de 5\%, não podemos rejeitar a hipótese nula para ambos os coeficientes usando o estimador ordinário.

\textbf{Testes de Hipótese - Estimador Generalizado:}
\begin{itemize}
    \item Estatística Z para $\beta_0$: 73.67
    \item p-valor para $\beta_0$: $< 10^{-15}$ (praticamente zero)
    \item p-valor para $\beta_1$: $< 10^{-15}$ (praticamente zero)
\end{itemize}

Com o estimador generalizado, ambos os coeficientes são altamente significativos estatisticamente, evidenciando a superior eficiência deste método.

\textbf{Conclusões:}
\begin{enumerate}
    \item O estimador de mínimos quadrados ordinários (OLS) falha completamente na presença de heterocedasticidade severa, fornecendo estimativas muito distantes dos valores verdadeiros ($\hat{\beta}_0 = -34.46$ vs $\beta_0 = 1.0$).

    \item O estimador generalizado (GLS) é extremamente superior, com erro cerca de 3.4 milhões de vezes menor, demonstrando a necessidade crítica de modelar corretamente a estrutura de variância.

    \item Os testes de significância baseados no OLS são não-confiáveis, falhando em detectar coeficientes que são claramente diferentes de zero.

    \item O estimador GLS fornece testes estatísticos apropriados, detectando corretamente a significância dos parâmetros.

    \item Este exemplo ilustra dramaticamente por que a correção para heterocedasticidade não é apenas uma melhoria técnica, mas uma necessidade fundamental para análise estatística válida.
\end{enumerate}

\newpage


\section{Exercício 3f}

\subsection{i) Implementa\c{c}\~ao dos Estimadores}

Neste exercício, comparamos estimadores baseados em diferentes suposições sobre a distribuição dos erros. Implementamos estimadores que minimizam diferentes funções de perda:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import scipy
import matplotlib.pyplot as plt

def beta_ordinary(X: np.ndarray, Y: np.ndarray) -> np.ndarray:
    """
    Compute the ordinary least squares estimator.
    """
    beta = np.linalg.inv(X.T @ X) @ X.T @ Y
    return beta

def calculate_beta_hat(X: np.ndarray, Y: np.ndarray, error_distribution: str) -> np.ndarray:
    """
    Compute the estimator beta_hat based on the specified error distribution.
    """
    np.random.seed(0)
    p = X.shape[1]
    
    if error_distribution == "gaussian":
        def loss_function(beta):
            return np.sum((Y - X @ beta) ** 2)
    elif error_distribution == "laplacian":
        def loss_function(beta):
            return np.sum(np.abs(Y - X @ beta))
    else:
        raise ValueError("Unsupported error distribution")
    
    beta_0 = np.random.uniform(size=p)
    beta_hat = scipy.optimize.minimize(loss_function, beta_0)
    return beta_hat["x"]
\end{lstlisting}

Geramos dados sintéticos para testar os estimadores:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
np.random.seed(1)
beta = np.array([-1.5, 2.0])
input_range = np.linspace(-1, 1, 100)
X = np.vstack([np.ones(100), input_range]).T
y = X @ beta + np.random.normal(0, 0.3, 100)
\end{lstlisting}

A Figura \ref{fig:data_scatter_q3} mostra os dados gerados:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/data_scatter.png}
    \caption{Dados sintéticos para comparação dos estimadores}
    \label{fig:data_scatter_q3}
\end{figure}

As Figuras \ref{fig:errors_hist_q3} e \ref{fig:errors_values_q3} mostram a análise dos erros:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/errors_histogram.png}
    \caption{Histograma dos erros}
    \label{fig:errors_hist_q3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/error_values_by_index.png}
    \caption{Valores dos erros por índice}
    \label{fig:errors_values_q3}
\end{figure}

\subsection{ii) Comparação dos Estimadores}

Calculamos os estimadores para ambas as distribuições de erro:

\textbf{Resultados sem Outliers:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [-1.5, 2.0]$
    \item Estimador Gaussiano (minimize): $\hat{\beta}_{Gauss} = [-1.482, 2.050]$
    \item Estimador Gaussiano (forma fechada): $\hat{\beta}_{OLS} = [-1.482, 2.050]$
    \item Estimador Laplaciano: $\hat{\beta}_{Lap} = [-1.498, 2.082]$
\end{itemize}

\textbf{Análise de Erro (Norma L2):}
\begin{itemize}
    \item Erro do Estimador Gaussiano: $0.053$
    \item Erro do Estimador Laplaciano: $0.082$
\end{itemize}

Sem outliers, o estimador gaussiano (mínimos quadrados) tem melhor performance, como esperado quando os erros seguem distribuição normal.

A Figura \ref{fig:regression_fits} compara visualmente os ajustes:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/regression_fits_comparison.png}
    \caption{Comparação dos ajustes de regressão sem outliers}
    \label{fig:regression_fits}
\end{figure}

\subsection{iii) Robustez a Outliers}

Para testar a robustez, adicionamos um outlier extremo ao dataset:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
# Regenerate data and add outlier
y[80] = 10  # Extreme outlier
\end{lstlisting}

\textbf{Resultados com Outlier:}
\begin{itemize}
    \item Parâmetros Verdadeiros: $\beta = [-1.5, 2.0]$
    \item Estimador Gaussiano com outlier: $\hat{\beta}_{Gauss} = [-1.378, 2.237]$
    \item Estimador Laplaciano com outlier: $\hat{\beta}_{Lap} = [-1.498, 2.083]$
\end{itemize}

\textbf{Análise de Erro com Outlier (Norma L2):}
\begin{itemize}
    \item Erro do Estimador Gaussiano: $0.266$ (aumento de 5x)
    \item Erro do Estimador Laplaciano: $0.083$ (praticamente inalterado)
\end{itemize}

A Figura \ref{fig:regression_fits_outlier} mostra o impacto do outlier:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/3/regression_fits_with_outlier.png}
    \caption{Comparação dos ajustes de regressão com outlier}
    \label{fig:regression_fits_outlier}
\end{figure}

\textbf{Conclusões:}
\begin{enumerate}
    \item \textbf{Eficiência}: Quando os erros são gaussianos e não há outliers, o estimador de mínimos quadrados (gaussiano) é mais eficiente.

    \item \textbf{Robustez}: O estimador laplaciano é significativamente mais robusto a outliers, mantendo sua performance praticamente inalterada mesmo com outliers extremos.

    \item \textbf{Trade-off}: Existe um trade-off entre eficiência (mínimos quadrados) e robustez (estimador laplaciano). A escolha depende das características esperadas dos dados.

    \item \textbf{Aplicação Prática}: Em situações onde outliers são esperados ou a distribuição dos erros tem caudas pesadas, o estimador laplaciano (regressão com norma L1) é preferível.

    \item \textbf{Impacto Dramático}: Um único outlier pode degradar significativamente a performance do estimador gaussiano (aumento de 5x no erro), enquanto o estimador laplaciano permanece praticamente inalterado.
\end{enumerate}

\newpage


\section{Exercício 4a}

\subsection{Implementação dos Algoritmos de Classificação}

Neste exercício, implementamos e comparamos cinco diferentes algoritmos de classificação usando o dataset de futebol:

\begin{itemize}
    \item \textbf{LDA} (Linear Discriminant Analysis)
    \item \textbf{QDA} (Quadratic Discriminant Analysis)
    \item \textbf{LR} (Logistic Regression)
    \item \textbf{NB} (Naive Bayes Gaussiano)
    \item \textbf{kNN} (k-Nearest Neighbors)
\end{itemize}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
from sklearn.linear_model import LogisticRegression as LR
from sklearn.naive_bayes import GaussianNB as NB
from sklearn.neighbors import KNeighborsClassifier as kNN
from sklearn import preprocessing

# Load and prepare data
df = pd.read_csv("../data/soccer.csv")
X = df.drop("target", axis=1)
y = df[["target"]]

# Split dataset
X_train, y_train = X.iloc[:2560], y.iloc[:2560]
X_test, y_test = X.iloc[2560:], y.iloc[2560:]

# Remove categorical variables and standardize
X_train = X_train.drop(["home_team", "away_team"], axis=1)
X_test = X_test.drop(["home_team", "away_team"], axis=1)
scaler = preprocessing.StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
\end{lstlisting}

\textbf{Informações do Dataset:}
\begin{itemize}
    \item Amostras de treino: 2560
    \item Amostras de teste: 640
    \item Features após pré-processamento: 11 (removendo variáveis categóricas)
\end{itemize}

\newpage

\section{Exercício 4b}

\subsection{Treinamento e Avaliação dos Modelos}

Implementamos um loop para treinar todos os modelos e comparar suas performances:

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
models_to_test = [LDA, QDA, LR, NB, kNN]
results_dict = {}

for model_type in models_to_test:
    model_name = model_type.__name__
    params = {}
    if model_type in [LDA, QDA]:
        params.update({"store_covariance": True})
    
    results_dict[model_name] = {}
    cls = model_type(**params)
    cls.fit(X_train, y_train.values.ravel())
    
    # Store predictions and model
    results_dict[model_name]["in_sample_predictions"] = cls.predict(X_train)
    results_dict[model_name]["test_predictions"] = cls.predict(X_test)
    results_dict[model_name]["model"] = cls
\end{lstlisting}

\subsection{Comparação de Performance dos Modelos}

\subsection{Análise Geral dos Algoritmos}

A Figura \ref{fig:models_comparison} compara os erros de treinamento e teste para todos os modelos:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/4/models_error_comparison.png}
    \caption{Comparação dos erros de treinamento vs teste para diferentes modelos}
    \label{fig:models_comparison}
\end{figure}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
for model_name in models_to_test:
    model_type_name = model_name.__name__
    in_sample_predictions = results_dict[model_type_name]["in_sample_predictions"]
    test_predictions = results_dict[model_type_name]["test_predictions"]
    
    train_error = np.mean(in_sample_predictions != y_train.values.ravel())
    test_error = np.mean(test_predictions != y_test.values.ravel())
    
    plt.scatter(train_error, test_error, label=model_type_name)
\end{lstlisting}

\subsection{Análise Específica do k-NN}

A Figura \ref{fig:knn_comparison} mostra como a performance do k-NN varia com diferentes valores de k:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/4/knn_error_comparison.png}
    \caption{Erros do k-NN para diferentes números de vizinhos (K=1 a K=10)}
    \label{fig:knn_comparison}
\end{figure}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
for k in range(1, 11):
    model = kNN(n_neighbors=k)
    model.fit(X_train, y_train.values.ravel())
    
    in_sample_predictions_k = model.predict(X_train)
    test_predictions_k = model.predict(X_test)
    
    train_error = np.mean(in_sample_predictions_k != y_train.values.ravel())
    test_error = np.mean(test_predictions_k != y_test.values.ravel())
    
    plt.scatter(train_error, test_error, label=f"K = {k}", marker=f"${k}$")
\end{lstlisting}

\subsection{Análise dos Coeficientes dos Modelos}

\textbf{Linear Discriminant Analysis (LDA):}
\begin{itemize}
    \item Coeficientes: $[0.81, -0.26, -0.026, 0.017, 0.23, 0.069, 0.37, -0.050, -0.083, 0.043, 0.047]$
    \item Intercepto: $0.109$
    \item Utiliza covariância comum entre as classes
\end{itemize}

\textbf{Logistic Regression:}
\begin{itemize}
    \item Coeficientes: $[0.87, -0.29, -0.020, 0.056, 0.26, 0.078, 0.40, -0.053, -0.078, 0.063, 0.047]$
    \item Intercepto: $0.128$
    \item Coeficientes similares ao LDA, indicando estrutura linear nos dados
\end{itemize}

\textbf{Gaussian Naive Bayes:}
\begin{itemize}
    \item Assume independência condicional entre features
    \item Médias das classes: $[-0.49, 0.28, 0.27, -0.30, -0.31, 0.23, -0.32, -0.18, 0.25, 0.17, 0.031]$ (Classe 0)
    \item Variâncias por feature calculadas separadamente para cada classe
\end{itemize}

\subsection{Conclusões}

\begin{enumerate}
    \item \textbf{Performance Geral}: Todos os modelos apresentaram performance similar, sugerindo que o problema tem estrutura linear bem definida.

    \item \textbf{Overfitting}: O k-NN com K=1 mostra clear overfitting (erro de treino muito baixo, erro de teste alto), enquanto valores maiores de K generalizam melhor.

    \item \textbf{Estabilidade}: LDA, QDA e Logistic Regression apresentaram performance consistente entre treino e teste.

    \item \textbf{Complexidade}: QDA, ao modelar covariâncias separadas por classe, não mostrou melhoria significativa sobre LDA, indicando que a suposição de covariância comum é adequada.

    \item \textbf{Naive Bayes}: Manteve performance competitiva mesmo com a forte suposição de independência, sugerindo que as correlações entre features não são críticas para este problema.

    \item \textbf{k-NN}: A performance otimizada ocorre com K entre 3-7, balanceando bias e variância.
\end{enumerate}

\newpage

\section{Exercício 5b}

\subsection{Métodos de Seleção de Modelos}

Neste exercício, implementamos e comparamos diferentes métodos de seleção de modelos para regressão linear usando o dataset de composição corporal (bodyfat). Os métodos implementados incluem:

\begin{itemize}
    \item \textbf{Best Subset Selection}: Avalia todas as combinações possíveis de features
    \item \textbf{Forward Stepwise Selection}: Adiciona features sequencialmente
    \item \textbf{Backward Stepwise Selection}: Remove features sequencialmente
\end{itemize}

\subsection{Preparação dos Dados}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
import statsmodels.api as sm
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.linear_model import Lasso
from sklearn import preprocessing

# Load and prepare data
bodyfat = pd.read_csv("../data/bodyfat.csv")
X = bodyfat.drop(columns=["BodyFat", "Density"])
y = bodyfat["BodyFat"]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=10
)

# Setup cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=10)
\end{lstlisting}

\subsection{Implementação dos Algoritmos}

\textbf{Best Subset Selection:}
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def best_subset_selection(X_train: np.ndarray, Y_train: np.ndarray) -> dict:
    """
    Perform best subset selection for linear regression. 
    Evaluates all possible combinations of features.
    """
    n, p = X_train.shape
    best_model_k = {}
    
    # Model with no features
    best_model_k["0"] = {
        "best_model": sm.OLS(Y_train, np.ones((len(Y_train), 1))).fit(),
        "best_r2": 0.0,
        "best_features": []
    }
    
    for k in range(1, p + 1):
        best_r2 = 0
        best_features = []
        best_model = []
        
        # Evaluate all combinations of k features
        for feature_combination in itertools.combinations(range(p), k):
            X_train_aux = sm.add_constant(X_train[:, feature_combination])
            model = sm.OLS(Y_train, X_train_aux).fit()
            
            if model.rsquared > best_r2:
                best_r2 = model.rsquared
                best_model = model
                best_features = feature_combination
        
        best_model_k[str(k)] = {
            "best_model": best_model,
            "best_r2": best_r2,
            "best_features": list(best_features),
        }
    
    return best_model_k
\end{lstlisting}

\textbf{Forward Stepwise Selection:}
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def forward_stepwise_selection(X_train: np.ndarray, Y_train: np.ndarray) -> dict:
    """
    Perform forward stepwise selection. Iteratively adds the best feature.
    """
    n, p = X_train.shape
    best_model_k = {}
    remaining_features = list(range(p))
    current_features = []
    
    # Model with no features
    best_model_k["0"] = {
        "best_model": sm.OLS(Y_train, np.ones((len(Y_train), 1))).fit(),
        "best_r2": 0.0,
        "best_features": []
    }
    
    for k in range(1, p + 1):
        best_r2 = 0
        best_features = []
        best_model = []
        
        # Try adding each remaining feature
        for new_feature in remaining_features:
            feature_combination_list = current_features + [new_feature]
            X_train_aux = sm.add_constant(X_train[:, feature_combination_list])
            model = sm.OLS(Y_train, X_train_aux).fit()
            
            if model.rsquared > best_r2:
                best_r2 = model.rsquared
                best_model = model
                best_features = feature_combination_list
        
        # Update feature lists
        remaining_features = list(set(remaining_features) - set(best_features))
        current_features = list(set(best_features + current_features))
        
        best_model_k[str(k)] = {
            "best_model": best_model,
            "best_r2": best_r2,
            "best_features": current_features.copy(),
        }
    
    return best_model_k
\end{lstlisting}

\textbf{Backward Stepwise Selection:}
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
def backward_stepwise_selection(X_train: np.ndarray, Y_train: np.ndarray) -> dict:
    """
    Perform backward stepwise selection. Iteratively removes the worst feature.
    """
    n, p = X_train.shape
    best_model_k = {}
    current_features = list(range(p))
    
    # Model with all features
    best_model_k[f"{p}"] = {
        "best_model": sm.OLS(Y_train, sm.add_constant(X_train)).fit(),
        "best_features": list(current_features)
    }
    best_model_k[f"{p}"]["best_r2"] = best_model_k[f"{p}"]["best_model"].rsquared
    
    for k in range(1, p + 1):
        best_r2 = 0
        best_features = []
        best_model = []
        
        # Try removing each current feature
        for feature_to_remove in current_features:
            feature_combination_list = current_features.copy()
            feature_combination_list.remove(feature_to_remove)
            X_train_aux = sm.add_constant(X_train[:, feature_combination_list])
            model = sm.OLS(Y_train, X_train_aux).fit()
            
            if model.rsquared > best_r2:
                best_r2 = model.rsquared
                best_model = model
                best_features = feature_combination_list
        
        current_features = best_features
        
        best_model_k[str(p - k)] = {
            "best_model": best_model,
            "best_r2": best_r2,
            "best_features": current_features.copy(),
        }
    
    return best_model_k
\end{lstlisting}

\newpage

\section{Exercício 5c}

\subsection{Comparação dos Métodos - R²}

A Figura \ref{fig:model_selection_r2} compara o desempenho dos três métodos em termos de R²:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/5/model_selection_methods_r2_comparison.png}
    \caption{Comparação dos métodos de seleção de modelos - R² vs Número de Features}
    \label{fig:model_selection_r2}
\end{figure}

\textbf{Resultados dos R² por Método:}
\begin{itemize}
    \item \textbf{1 feature}: R² = 0.640 (feature: Abdomen)
    \item \textbf{2 features}: R² = 0.694 (features: Weight, Abdomen)
    \item \textbf{3 features}: R² = 0.710 (adicionando uma terceira feature)
    \item \textbf{Todos os métodos convergem}: Para 1-3 features, todos os métodos encontram as mesmas soluções ótimas
    \item \textbf{Divergência}: A partir de 4+ features, backward stepwise pode encontrar soluções ligeiramente diferentes
\end{itemize}

\newpage

\section{Exercício 5d}

\subsection{Regressão Lasso com Validação Cruzada}

\subsection{Seleção do Parâmetro de Regularização}

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily, breaklines=true]
# Cross-validation for Lasso
alphas = 10 ** np.linspace(5, -2, 100)
mean_cv_error = {}

for alpha_0 in alphas:
    fold_error = []
    for test_fold in np.unique(cv_fold):
        # Split data for current fold
        x_train_fold = X_train[cv_fold != test_fold]
        y_train_fold = y_train[cv_fold != test_fold]
        x_test_fold = X_train[cv_fold == test_fold]
        y_test_fold = y_train[cv_fold == test_fold]
        
        # Normalize data
        x_train_fold, x_test_fold = normalize_data(x_train_fold, x_test_fold)
        
        # Train and evaluate Lasso model
        model_ = Lasso(alpha=alpha_0).fit(x_train_fold, y_train_fold)
        yhat = model_.predict(x_test_fold)
        fold_error.append(mean_squared_error(yhat, y_test_fold))
    
    mean_cv_error[alpha_0] = np.mean(fold_error)

best_alpha = min(mean_cv_error, key=mean_cv_error.get)
\end{lstlisting}

A Figura \ref{fig:lasso_cv} mostra a curva de validação cruzada para o Lasso:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/5/lasso_cv_error_vs_alpha.png}
    \caption{Lasso Regression: Erro de Validação Cruzada vs Parâmetro Alpha}
    \label{fig:lasso_cv}
\end{figure}

\textbf{Resultados do Lasso:}
\begin{itemize}
    \item \textbf{Melhor Alpha}: $\alpha = 0.0313$
    \item \textbf{Erro de CV mínimo}: 21.12
    \item \textbf{Features selecionadas}: Todas (coeficientes não-zero para todas as 13 features)
    \item \textbf{Maior coeficiente}: Abdomen (10.04) - confirma sua importância
\end{itemize}

\newpage

\section{Exercício 5e}

\subsection{Comparação no Conjunto de Teste}

A Figura \ref{fig:test_errors} compara o erro no conjunto de teste para todos os métodos:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/5/model_selection_test_error_comparison.png}
    \caption{Compara\c{c}\~ao dos erros no conjunto de teste para diferentes métodos}
    \label{fig:test_errors}
\end{figure}

\subsection{Resultados Numéricos}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        Features & Best Subset MSE & Forward Stepwise MSE & Backward Stepwise MSE \\
        \hline
        1        & 20.01           & 20.01                & 20.01                 \\
        2        & 14.58           & 14.58                & 14.58                 \\
        3        & 16.16           & 16.16                & 16.16                 \\
        6        & 16.32           & 16.32                & 16.06                 \\
        7        & 15.72           & 16.92                & 15.72                 \\
        9        & 15.94           & 15.94                & 15.94                 \\
        \hline
        \multicolumn{4}{|c|}{Lasso Regression MSE: \textbf{15.56}}                \\
        \hline
    \end{tabular}
    \caption{Erros de teste (MSE) para diferentes números de features}
\end{table}

\subsection{Análise do Modelo de 2 Features}

O modelo com 2 features (Weight e Abdomen) mostra excelente performance:

\textbf{Modelo}: $\text{BodyFat} = -45.63 - 0.14 \times \text{Weight} + 0.98 \times \text{Abdomen}$

\begin{itemize}
    \item \textbf{Intercepto}: -45.63
    \item \textbf{Coef. Weight}: -0.14 (negativo - mais peso, menos gordura corporal relativa)
    \item \textbf{Coef. Abdomen}: 0.98 (positivo - maior circunferência abdominal, maior gordura corporal)
    \item \textbf{R²}: 0.694
    \item \textbf{MSE no teste}: 14.58
\end{itemize}

\subsection{Conclusões}

\begin{enumerate}
    \item \textbf{Convergência dos Métodos}: Para números pequenos de features (1-3), todos os métodos stepwise encontram as mesmas soluções ótimas que o best subset selection.

    \item \textbf{Feature Mais Importante}: Abdomen (circunferência abdominal) é consistentemente a feature mais importante, explicando 64\% da variância sozinha.

    \item \textbf{Modelo Parcimonioso}: O modelo de 2 features (Weight + Abdomen) oferece excelente trade-off entre simplicidade e performance (MSE = 14.58).

    \item \textbf{Lasso Performance}: O Lasso alcança o melhor resultado no teste (MSE = 15.56), mas usa todas as features. O trade-off é menor interpretabilidade.

    \item \textbf{Overfitting}: Modelos com mais de 3 features mostram sinais de overfitting, com R² crescente no treino mas MSE crescente no teste.

    \item \textbf{Eficiência Computacional}: Forward/backward stepwise são muito mais eficientes que best subset ($O(p^2)$ vs $O(2^p)$), com performance quase idêntica para este problema.

    \item \textbf{Regularização}: O Lasso fornece uma abordagem automática para seleção de features com excelente performance de generalização.
\end{enumerate}

\newpage


\end{document}